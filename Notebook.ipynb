{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a7b8c9",
   "metadata": {},
   "source": [
    "# Ejercisios Practicos IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "Imports Ej 6 práctico 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from deap import base, creator, tools, algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "Ejercicio 6 Práctica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANT_CIUDADES = 20\n",
    "random.seed(42)\n",
    "\n",
    "# Generamos coordenadas aleatorias para las ciudades\n",
    "CIUDADES = [(random.randint(0, 100), random.randint(0, 100)) for _ in range(CANT_CIUDADES)]\n",
    "\n",
    "# Matriz de distancias\n",
    "MATRIZ_DISTANCIAS = {}\n",
    "for i in range(CANT_CIUDADES):\n",
    "    MATRIZ_DISTANCIAS[i] = {}\n",
    "    for j in range(CANT_CIUDADES):\n",
    "        MATRIZ_DISTANCIAS[i][j] = math.dist(CIUDADES[i], CIUDADES[j])\n",
    "\n",
    "# Matriz de tiempos\n",
    "MATRIZ_TIEMPOS = {}\n",
    "for i in range(CANT_CIUDADES):\n",
    "    MATRIZ_TIEMPOS[i] = {}\n",
    "    for j in range(CANT_CIUDADES):\n",
    "        if i == j:\n",
    "            MATRIZ_TIEMPOS[i][j] = 0\n",
    "        else:\n",
    "            trafico = random.uniform(0.8, 2.5)\n",
    "            MATRIZ_TIEMPOS[i][j] = MATRIZ_DISTANCIAS[i][j] * trafico\n",
    "\n",
    "\n",
    "# Como el problema tiene distancia y tiempo como objectivos\n",
    "# le damos dos pesos negativos (minimización en ambos).\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, -1.0))\n",
    "creator.create(\"Individuo\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Un individuo es una permutación de todas las ciudades\n",
    "toolbox.register(\"indices_ciudades\", random.sample, range(CANT_CIUDADES), CANT_CIUDADES)\n",
    "toolbox.register(\"individuo\", tools.initIterate, creator.Individuo, toolbox.indices_ciudades)\n",
    "toolbox.register(\"poblacion\", tools.initRepeat, list, toolbox.individuo)\n",
    "\n",
    "\n",
    "def evaluar_ruta_multi(individuo):\n",
    "    dist_total = 0\n",
    "    tiempo_total = 0\n",
    "\n",
    "    ciudad_actual = individuo[0]\n",
    "    for siguiente in individuo[1:]:\n",
    "        dist_total += MATRIZ_DISTANCIAS[ciudad_actual][siguiente]\n",
    "        tiempo_total += MATRIZ_TIEMPOS[ciudad_actual][siguiente]\n",
    "        ciudad_actual = siguiente\n",
    "\n",
    "    # Cerramos el ciclo volviendo a la primera ciudad\n",
    "    dist_total += MATRIZ_DISTANCIAS[ciudad_actual][individuo[0]]\n",
    "    tiempo_total += MATRIZ_TIEMPOS[ciudad_actual][individuo[0]]\n",
    "\n",
    "    return dist_total, tiempo_total\n",
    "\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluar_ruta_multi)\n",
    "toolbox.register(\"mate\", tools.cxPartialyMatched)\n",
    "toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selNSGA2)  # selección multiobjetivo (NSGA-II)\n",
    "\n",
    "\n",
    "def ejecutar_moga_tsp():\n",
    "    print(\"\\n Problema del Agente Viajero con MOGA\\n \")\n",
    "\n",
    "    TAM_POBLACION = 300\n",
    "    PROB_CRUCE = 0.7\n",
    "    PROB_MUTACION = 0.2\n",
    "    MAX_GENERACIONES = 250\n",
    "\n",
    "    poblacion = toolbox.poblacion(n=TAM_POBLACION)\n",
    "    frente_pareto = tools.ParetoFront()\n",
    "\n",
    "    # Estadísticas separadas para distancia y tiempo\n",
    "    stats_dist = tools.Statistics(lambda ind: ind.fitness.values[0])\n",
    "    stats_dist.register(\"avg\", lambda x: sum(x) / len(x))\n",
    "    stats_dist.register(\"min\", min)\n",
    "\n",
    "    stats_tiempo = tools.Statistics(lambda ind: ind.fitness.values[1])\n",
    "    stats_tiempo.register(\"avg\", lambda x: sum(x) / len(x))\n",
    "    stats_tiempo.register(\"min\", min)\n",
    "\n",
    "    stats = tools.MultiStatistics(distancia=stats_dist, tiempo=stats_tiempo)\n",
    "\n",
    "    poblacion, log = algorithms.eaMuPlusLambda(\n",
    "        poblacion, toolbox,\n",
    "        mu=TAM_POBLACION, lambda_=TAM_POBLACION,\n",
    "        cxpb=PROB_CRUCE, mutpb=PROB_MUTACION,\n",
    "        ngen=MAX_GENERACIONES,\n",
    "        stats=stats, halloffame=frente_pareto, verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Evolución completada ---\")\n",
    "    print(f\"Se encontraron {len(frente_pareto)} soluciones no dominadas (frente de Pareto):\")\n",
    "    print(\"(Distancia, Tiempo)\")\n",
    "\n",
    "    for ind in frente_pareto:\n",
    "        d, t = ind.fitness.values\n",
    "        print(f\"  {d:.2f}, {t:.2f}\")\n",
    "\n",
    "    print(\"\\nPrimera solución del frente (la más corta en distancia):\")\n",
    "    d, t = frente_pareto[0].fitness.values\n",
    "    print(f\"  Ruta: {frente_pareto[0]}\")\n",
    "    print(f\"  Distancia total: {d:.2f}\")\n",
    "    print(f\"  Tiempo total: {t:.2f}\")\n",
    "\n",
    "\n",
    "# Ejecutamos la función para que corra en el notebook\n",
    "ejecutar_moga_tsp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b94be",
   "metadata": {},
   "source": [
    "Imports Ej 4 práctico 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f199f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89c729",
   "metadata": {},
   "source": [
    "Ejercicio 4 práctica 2, auto-mpg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703035af",
   "metadata": {},
   "outputs": [],
   "source": [
    "carsConsume = pd.read_csv(Path(\"Datasets/auto-mpg[1].csv\"))\n",
    "carsConsume.hist(bins=50, figsize=(12,8))\n",
    "plt.show()\n",
    "print(carsConsume.head())\n",
    "print(carsConsume.columns)\n",
    "\n",
    "# Reemplazar \"?\" por NaN y convertir horsepower a numérico\n",
    "carsConsume[\"horsepower\"].replace(\"?\", np.nan, inplace=True)\n",
    "carsConsume[\"horsepower\"] = carsConsume[\"horsepower\"].astype(float)\n",
    "\n",
    "print(carsConsume.info())\n",
    "\n",
    "y = carsConsume[\"mpg\"]\n",
    "X = carsConsume.drop(columns=[\"mpg\", \"car name\"], errors='ignore')\n",
    "\n",
    "train_carsConsume, test_carsConsume = train_test_split(carsConsume,test_size=0.2, stratify=carsConsume[\"cylinders\"], random_state=42)\n",
    "\n",
    "X_train = train_carsConsume.drop(columns=[\"mpg\", \"car name\"], errors='ignore')\n",
    "y_train = train_carsConsume[\"mpg\"]\n",
    "\n",
    "X_test = test_carsConsume.drop(columns=[\"mpg\", \"car name\"], errors='ignore')\n",
    "y_test = test_carsConsume[\"mpg\"]\n",
    "\n",
    "carsConsume.plot(kind=\"scatter\", x=\"weight\", y=\"horsepower\", grid=True, figsize=(8, 6))\n",
    "plt.title(\"Relación entre peso y potencia\")\n",
    "plt.show()\n",
    "\n",
    "attributes = [\"mpg\", \"horsepower\", \"weight\", \"displacement\", \"acceleration\"]\n",
    "scatter_matrix(carsConsume[attributes], figsize=(12, 8), alpha=0.7, diagonal='hist')\n",
    "plt.suptitle(\"Matriz de dispersión — Dataset Auto MPG\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Relación potencia/peso — eficiencia del motor\n",
    "carsConsume[\"power_weight_ratio\"] = carsConsume[\"horsepower\"] / carsConsume[\"weight\"]\n",
    "\n",
    "# Relación cilindrada/peso — mide \"grandeza del motor relativo al tamaño del auto\"\n",
    "carsConsume[\"displacement_per_weight\"] = carsConsume[\"displacement\"] / carsConsume[\"weight\"]\n",
    "\n",
    "# Relación aceleración/potencia — qué tan rápido acelera por caballo de fuerza\n",
    "carsConsume[\"acceleration_per_hp\"] = carsConsume[\"acceleration\"] / carsConsume[\"horsepower\"]\n",
    "\n",
    "corr_matrix = carsConsume.corr(numeric_only=True)\n",
    "\n",
    "# Ordenar correlaciones con respecto a 'mpg'\n",
    "print(corr_matrix[\"mpg\"].sort_values(ascending=False))\n",
    "\n",
    "median_hp = carsConsume[\"horsepower\"].median()\n",
    "carsConsume[\"horsepower\"].fillna(median_hp, inplace=True)\n",
    "\n",
    "num_attribs = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"]\n",
    "cat_attribs = [\"origin\"]\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([(\"num\", num_pipeline, num_attribs),(\"cat\", cat_pipeline, cat_attribs)])\n",
    "\n",
    "set_config(display='diagram')  # Muestra el pipeline visualmente\n",
    "carsConsume_prepared = preprocessing.fit_transform(carsConsume)\n",
    "\n",
    "print(\"Shape del dataset procesado:\", carsConsume_prepared.shape)\n",
    "\n",
    "X_train_prepared = preprocessing.fit_transform(X_train)\n",
    "X_test_prepared = preprocessing.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_prepared, y_train)\n",
    "y_pred = lin_reg.predict(X_test_prepared)\n",
    "\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.3f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
    "\n",
    "carsConsume_predictions = lin_reg.predict(X_test_prepared)\n",
    "print(\"Primeras 5 predicciones:\", carsConsume_predictions[:5].round(1))\n",
    "print(\"Valores reales:\", y_test.iloc[:5].values)\n",
    "\n",
    "lin_mse = mean_squared_error(y_test, carsConsume_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(\"Linear Regression RMSE:\", lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-p3-ej6",
   "metadata": {},
   "source": [
    "Imports Ej 6 Práctica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-imports-p3-ej6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title-p3-ej6",
   "metadata": {},
   "source": [
    "Ejercicio 6 Práctica 3 - Clasificación Multiclase (Cervezas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-p3-ej6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Cada fila: [IBU, SRM]\n",
    "n = 50\n",
    "ipa = np.column_stack((np.random.normal(55, 5, n), np.random.normal(20, 5, n)))         # Clara pero amarga\n",
    "lager = np.column_stack((np.random.normal(15, 3, n), np.random.normal(10, 3, n)))       # Clara y poco amarga\n",
    "scottish = np.column_stack((np.random.normal(30, 5, n), np.random.normal(40, 5, n)))    # Oscura pero moderada\n",
    "stout = np.column_stack((np.random.normal(60, 5, n), np.random.normal(70, 5, n)))       # Muy oscura y amarga\n",
    "\n",
    "X = np.vstack((lager, stout, ipa, scottish))\n",
    "y = np.array([0]*n + [1]*n + [2]*n + [3]*n)\n",
    "\n",
    "clases = ['Lager', 'Stout', 'IPA', 'Scottish']\n",
    "\n",
    "# dividimos entre entrenamiento y validacion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Escalado de características\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Dataset original\n",
    "plt.figure(figsize=(7,6))\n",
    "colores = ['gold', 'brown', 'orange', 'purple']\n",
    "for i, c in zip(range(4), colores):\n",
    "    plt.scatter(X[y==i, 0], X[y==i, 1], label=clases[i], color=c)\n",
    "plt.xlabel(\"IBU (Amargor)\")\n",
    "plt.ylabel(\"SRM (Color)\")\n",
    "plt.title(\"Dataset sintético - Estilos de cerveza\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# CLASIFICADORES MULTICLASE\n",
    "\n",
    "# One vs Rest \n",
    "modelo_ovr = OneVsRestClassifier(LogisticRegression(max_iter=500))\n",
    "modelo_ovr.fit(X_train_std, y_train)\n",
    "pred_ovr = modelo_ovr.predict(X_test_std)\n",
    "\n",
    "print(\"One vs Rest (OVR) \")\n",
    "print(classification_report(y_test, pred_ovr, target_names=clases))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, pred_ovr), display_labels=clases).plot(cmap='Blues')\n",
    "plt.title(\"Matriz de confusión - OVR\")\n",
    "plt.show()\n",
    "\n",
    "# One vs One\n",
    "modelo_ovo = OneVsOneClassifier(LogisticRegression(max_iter=500))\n",
    "modelo_ovo.fit(X_train_std, y_train)\n",
    "pred_ovo = modelo_ovo.predict(X_test_std)\n",
    "\n",
    "print(\"One vs One (OVO)\")\n",
    "print(classification_report(y_test, pred_ovo, target_names=clases))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, pred_ovo), display_labels=clases).plot(cmap='Greens')\n",
    "plt.title(\"Matriz de confusión - OVO\")\n",
    "plt.show()\n",
    "\n",
    "# Softmax multinomial \n",
    "modelo_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
    "modelo_softmax.fit(X_train_std, y_train)\n",
    "pred_softmax = modelo_softmax.predict(X_test_std)\n",
    "\n",
    "print(\" Softmax multinomial \")\n",
    "print(classification_report(y_test, pred_softmax, target_names=clases))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, pred_softmax), display_labels=clases).plot(cmap='Oranges')\n",
    "plt.title(\"Matriz de confusión - Softmax\")\n",
    "plt.show()\n",
    "\n",
    "# Comparacion de hiperparametros\n",
    "print(\"\\nComparación de hiperparámetros (Softmax multinomial) \")\n",
    "for c in [0.1, 1, 10]:\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500, C=c)\n",
    "    model.fit(X_train_std, y_train)\n",
    "    score = model.score(X_test_std, y_test)\n",
    "    print(f\"C={c:<4} -> Precisión de validación: {score:.3f}\")\n",
    "\n",
    "# Fronteras de decision\n",
    "def mostrar_fronteras(modelo, X, y, titulo):\n",
    "    paso = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, paso),\n",
    "                         np.arange(y_min, y_max, paso))\n",
    "    Z = modelo.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.tab10)\n",
    "    for i, c in zip(range(4), colores):\n",
    "        plt.scatter(X[y==i, 0], X[y==i, 1], label=clases[i], color=c)\n",
    "    plt.xlabel(\"IBU (Amargor)\")\n",
    "    plt.ylabel(\"SRM (Color)\")\n",
    "    plt.title(titulo)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Fronteras de decisión para cada modelo\n",
    "mostrar_fronteras(modelo_ovr, X, y, \"Fronteras de decisión - One vs Rest\")\n",
    "mostrar_fronteras(modelo_ovo, X, y, \"Fronteras de decisión - One vs One\")\n",
    "mostrar_fronteras(modelo_softmax, X, y, \"Fronteras de decisión - Softmax multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc61f8",
   "metadata": {},
   "source": [
    "Imports Ej 7 práctica 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025961f1",
   "metadata": {},
   "source": [
    "Ej 7 Práctica 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8108f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_vs_MW = pd.read_csv('Datasets/ScreenTime vs MentalWellness.csv')\n",
    "\n",
    "def categorize_wellness(score):\n",
    "    if score <= 33:\n",
    "        return 'Bajo'\n",
    "    elif score <= 66:\n",
    "        return 'Medio'\n",
    "    else:\n",
    "        return 'Alto'\n",
    "\n",
    "ST_vs_MW['wellness_class'] = ST_vs_MW['mental_wellness_index_0_100'].apply(categorize_wellness)\n",
    "\n",
    "ST_vs_MW = ST_vs_MW.drop(columns=['user_id', 'mental_wellness_index_0_100'])\n",
    "\n",
    "categorical_cols = ['gender', 'occupation', 'work_mode']\n",
    "numerical_cols = [col for col in ST_vs_MW.columns if col not in categorical_cols + ['wellness_class']]\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_cats = encoder.fit_transform(ST_vs_MW[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "X = pd.concat([ST_vs_MW[numerical_cols].reset_index(drop=True), encoded_df], axis=1)\n",
    "y = ST_vs_MW['wellness_class'].astype(str).values  # Asegura tipo string\n",
    "\n",
    "# Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Training\n",
    "ST_vs_MW_DT = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "ST_vs_MW_DT.fit(X_train, y_train)\n",
    "\n",
    "ST_vs_MW_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ST_vs_MW_RF.fit(X_train, y_train)\n",
    "\n",
    "# Predicts\n",
    "y_pred_dt = ST_vs_MW_DT.predict(X_test)\n",
    "y_pred_rf = ST_vs_MW_RF.predict(X_test)\n",
    "\n",
    "# Matrices de confusión\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision Tree\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt, labels=['Bajo', 'Medio', 'Alto'])\n",
    "disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=['Bajo', 'Medio', 'Alto'])\n",
    "disp_dt.plot(ax=axes[0], cmap='Blues', colorbar=False)\n",
    "axes[0].set_title('Matriz de Confusión - Decision Tree')\n",
    "\n",
    "# Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf, labels=['Bajo', 'Medio', 'Alto'])\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Bajo', 'Medio', 'Alto'])\n",
    "disp_rf.plot(ax=axes[1], cmap='Greens', colorbar=False)\n",
    "axes[1].set_title('Matriz de Confusión - Random Forest')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Métricas numéricas\n",
    "print(\"=== Métricas de Clasificación ===\")\n",
    "for name, y_pred in [(\"Decision Tree\", y_pred_dt), (\"Random Forest\", y_pred_rf)]:\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  F1-score: {f1:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall: {rec:.4f}\")\n",
    "\n",
    "# Comparación de métricas\n",
    "metrics = ['Accuracy', 'F1-score', 'Precision', 'Recall']\n",
    "dt_vals = [\n",
    "    accuracy_score(y_test, y_pred_dt),\n",
    "    f1_score(y_test, y_pred_dt, average='weighted'),\n",
    "    precision_score(y_test, y_pred_dt, average='weighted'),\n",
    "    recall_score(y_test, y_pred_dt, average='weighted')\n",
    "]\n",
    "rf_vals = [\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "    precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "    recall_score(y_test, y_pred_rf, average='weighted')\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x - width/2, dt_vals, width, label='Decision Tree', color='lightsteelblue', edgecolor='black')\n",
    "plt.bar(x + width/2, rf_vals, width, label='Random Forest', color='lightgreen', edgecolor='black')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylabel('Puntuación')\n",
    "plt.title('Comparación de Métricas de Clasificación')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Importancia de características (solo Random Forest)\n",
    "importances = ST_vs_MW_RF.feature_importances_\n",
    "indices = np.argsort(importances)[::-1][:10]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(range(10), importances[indices], color='seagreen', edgecolor='black')\n",
    "plt.yticks(range(10), [X.columns[i] for i in indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 10: Importancia de Características (Random Forest)')\n",
    "plt.xlabel('Importancia')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-p4-ej9",
   "metadata": {},
   "source": [
    "Imports Ej 9 Práctica 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-imports-p4-ej9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title-p4-ej9",
   "metadata": {},
   "source": [
    "Ejercicio 9 Práctica 4 - Stacking Classifier (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-p4-ej9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "labels = data.target_names\n",
    "\n",
    "# Dividir entre entrenamiento y validación \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalar características \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelos base \n",
    "base_models = [\n",
    "    ('dt', DecisionTreeClassifier(max_depth=4, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('logreg', LogisticRegression(max_iter=500))\n",
    "]\n",
    "\n",
    "# Distintos meta-modelos para probar mejoras \n",
    "meta_models = {\n",
    "    'Regresión logística': LogisticRegression(max_iter=500),\n",
    "    'SVM': SVC(kernel='linear', probability=True),\n",
    "    'Árbol de decisión': DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar los modelos de Stacking \n",
    "for name, meta_model in meta_models.items():\n",
    "    print(f\"\\nMeta-modelo: {name} \")\n",
    "    stack = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        passthrough=True\n",
    "    )\n",
    "    stack.fit(X_train_scaled, y_train)\n",
    "    y_pred = stack.predict(X_test_scaled)\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=labels))\n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=labels).plot(cmap='Blues')\n",
    "    plt.title(f\"Matriz de confusión - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Comparación visual de precisión\n",
    "scores = []\n",
    "for name, meta_model in meta_models.items():\n",
    "    stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, passthrough=True)\n",
    "    stack.fit(X_train_scaled, y_train)\n",
    "    acc = stack.score(X_test_scaled, y_test)\n",
    "    scores.append((name, acc))\n",
    "\n",
    "names, accs = zip(*scores)\n",
    "plt.bar(names, accs, color=['orange', 'green', 'blue'])\n",
    "plt.ylabel(\"Precisión de validación\")\n",
    "plt.title(\"Comparación de meta-modelos en Stacking\")\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb75e7",
   "metadata": {},
   "source": [
    "Imports Ej 3 Práctica 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0dbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e29e59",
   "metadata": {},
   "source": [
    "Ej 3 Práctica 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# División en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Escalado de datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Árbol de decisión\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"RESULTADOS \\n\")\n",
    "print(\"MLP:\", round(accuracy_score(y_test, y_pred_mlp), 3))\n",
    "print(\"SVM:\", round(accuracy_score(y_test, y_pred_svm), 3))\n",
    "print(\"Decision Tree:\", round(accuracy_score(y_test, y_pred_tree), 3), \"\\n\")\n",
    "\n",
    "print(\"CLASIFICACIÓN (MLP)\\n\")\n",
    "print(classification_report(y_test, y_pred_mlp, target_names=target_names))\n",
    "\n",
    "# Matriz de confusión con Matplotlib\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "models = {\n",
    "    \"MLP\": y_pred_mlp,\n",
    "    \"SVM\": y_pred_svm,\n",
    "    \"Decision Tree\": y_pred_tree\n",
    "}\n",
    "\n",
    "for ax, (name, y_pred) in zip(axes, models.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "    disp.plot(ax=ax, colorbar=False)\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-p6-ej3",
   "metadata": {},
   "source": [
    "Imports Ej 3 Práctica 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-imports-p6-ej3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "# Imports para contraste\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title-p6-ej3",
   "metadata": {},
   "source": [
    "Ejercicio 3 Práctica 6 - CNN (Iris) y Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-p6-ej3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset \n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "labels = iris.target_names\n",
    "\n",
    "# Escalado y división\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# NOTA: Para el contraste, el Árbol de Decisión usará X_train/X_test no escalados.\n",
    "# Creamos esas variables aquí para tenerlas disponibles.\n",
    "X_train_no_scaled, X_test_no_scaled, y_train_no_scaled, y_test_no_scaled = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Adaptar datos a formato CNN (2D)\n",
    "# CNN espera datos con forma (samples, height, width, channels)\n",
    "# Convertimos cada muestra (4 características) a una \"imagen\" de 2x2 con 1 canal.\n",
    "X_train_cnn = X_train.reshape(-1, 2, 2, 1)\n",
    "X_test_cnn = X_test.reshape(-1, 2, 2, 1)\n",
    "\n",
    "# One-hot encoding de etiquetas\n",
    "y_train_cat = keras.utils.to_categorical(y_train)\n",
    "y_test_cat = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Función para crear distintos modelos CNN\n",
    "def create_cnn_model(conv_layers=1, dense_units=32):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(8, (2, 2), activation='relu', input_shape=(2, 2, 1)))\n",
    "    \n",
    "    # Agregar más capas convolucionales según configuración\n",
    "    for _ in range(conv_layers - 1):\n",
    "        model.add(layers.Conv2D(8, (2, 2), activation='relu'))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Distintas configuraciones a probar \n",
    "configs = [\n",
    "    {\"conv_layers\": 1, \"dense_units\": 16},\n",
    "    {\"conv_layers\": 2, \"dense_units\": 32},\n",
    "    {\"conv_layers\": 2, \"dense_units\": 64},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "for cfg in configs:\n",
    "    print(f\"\\n=== Modelo: {cfg['conv_layers']} conv, {cfg['dense_units']} dense ===\")\n",
    "    model = create_cnn_model(cfg[\"conv_layers\"], cfg[\"dense_units\"])\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_cnn, y_train_cat,\n",
    "        validation_data=(X_test_cnn, y_test_cat),\n",
    "        epochs=30,\n",
    "        batch_size=8,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test_cnn, y_test_cat, verbose=0)\n",
    "    print(f\"Precisión en test: {test_acc:.4f}\")\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test_cnn), axis=1)\n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=labels).plot(cmap='Blues')\n",
    "    plt.title(f\"{cfg['conv_layers']} conv, {cfg['dense_units']} dense - Matriz de confusión\")\n",
    "    plt.show()\n",
    "    \n",
    "    results.append({\n",
    "        \"conv_layers\": cfg[\"conv_layers\"],\n",
    "        \"dense_units\": cfg[\"dense_units\"],\n",
    "        \"accuracy\": test_acc,\n",
    "        \"history\": history\n",
    "    })\n",
    "\n",
    "# Comparación de resultados\n",
    "plt.figure(figsize=(8,5))\n",
    "for res in results:\n",
    "    plt.plot(res[\"history\"].history['val_accuracy'], label=f\"{res['conv_layers']} conv / {res['dense_units']} dense\")\n",
    "plt.title(\"Comparación de precisión en validación\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Resumen de configuraciones CNN \")\n",
    "for res in results:\n",
    "    print(f\"{res['conv_layers']} conv / {res['dense_units']} dense -> Precisión: {res['accuracy']:.3f}\")\n",
    "\n",
    "# Contraste con modelos previos \n",
    "print(\"\\n--- Contraste con modelos previos (cálculo real) ---\")\n",
    "\n",
    "# 1. Árbol de Decisión (como en Práctica 5)\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree.fit(X_train_no_scaled, y_train_no_scaled) # Usa datos no escalados\n",
    "y_pred_tree = tree.predict(X_test_no_scaled)\n",
    "acc_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\" - Árbol de Decisión: {acc_tree:.4f}\")\n",
    "\n",
    "# 2. KNN (K-Nearest Neighbors)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train) # X_train está escalado\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\" - KNN: {acc_knn:.4f}\")\n",
    "\n",
    "# 3. Regresión Logística (Softmax)\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_logreg = log_reg.predict(X_test)\n",
    "acc_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\" - Regresión Logística: {acc_logreg:.4f}\")\n",
    "\n",
    "# 4. Stacking (como en Práctica 4, Ej 9)\n",
    "base_models_stack = [\n",
    "    ('dt', DecisionTreeClassifier(max_depth=4, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('logreg', LogisticRegression(max_iter=500, random_state=42))\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_models_stack,\n",
    "    final_estimator=LogisticRegression(max_iter=500),\n",
    "    passthrough=True\n",
    ")\n",
    "stack.fit(X_train, y_train)\n",
    "y_pred_stack = stack.predict(X_test)\n",
    "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
    "print(f\" - Stacking (mejor caso): {acc_stack:.4f}\")\n",
    "\n",
    "# 5. CNN\n",
    "best_cnn_acc = max(r['accuracy'] for r in results)\n",
    "print(f\" - CNN (mejor configuración actual): {best_cnn_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b48ab6",
   "metadata": {},
   "source": [
    "Imports Ej 4 Práctica 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563fbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c938af",
   "metadata": {},
   "source": [
    "Ej 4 Práctica 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Datasets/poems.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters for GRU\n",
    "# update gate\n",
    "Wz = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Uz = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "bz = np.zeros((hidden_size, 1))\n",
    "# reset gate\n",
    "Wr = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Ur = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "br = np.zeros((hidden_size, 1))\n",
    "# candidate hidden state\n",
    "Wh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Uh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "# output layer\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "by = np.zeros((vocab_size, 1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets: listas de índices de caracteres\n",
    "    hprev: estado oculto inicial (Hx1)\n",
    "    Devuelve: loss, gradientes y último hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, zs, rs, h_tilde, ys, ps = {}, {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "\n",
    "    # ---------- FORWARD ----------\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "\n",
    "        # GRU equations\n",
    "        zs[t] = sigmoid(np.dot(Wz, xs[t]) + np.dot(Uz, hs[t - 1]) + bz)\n",
    "        rs[t] = sigmoid(np.dot(Wr, xs[t]) + np.dot(Ur, hs[t - 1]) + br)\n",
    "        h_tilde[t] = np.tanh(np.dot(Wh, xs[t]) + np.dot(Uh, rs[t] * hs[t - 1]) + bh)\n",
    "        hs[t] = (1 - zs[t]) * hs[t - 1] + zs[t] * h_tilde[t]\n",
    "\n",
    "        # output layer\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "\n",
    "    # ---------- BACKWARD ----------\n",
    "    dWz, dUz, dbz = np.zeros_like(Wz), np.zeros_like(Uz), np.zeros_like(bz)\n",
    "    dWr, dUr, dbr = np.zeros_like(Wr), np.zeros_like(Ur), np.zeros_like(br)\n",
    "    dWh, dUh, dbh = np.zeros_like(Wh), np.zeros_like(Uh), np.zeros_like(bh)\n",
    "    dWhy, dby = np.zeros_like(Why), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        dz = (h_tilde[t] - hs[t - 1]) * dh\n",
    "        dz *= zs[t] * (1 - zs[t])\n",
    "\n",
    "        dh_tilde = zs[t] * dh\n",
    "        dh_tilde_raw = (1 - h_tilde[t] * h_tilde[t]) * dh_tilde\n",
    "\n",
    "        dr = np.dot(Uh.T, dh_tilde_raw) * hs[t - 1]\n",
    "        dr *= rs[t] * (1 - rs[t])\n",
    "\n",
    "        dWz += np.dot(dz, xs[t].T)\n",
    "        dUz += np.dot(dz, hs[t - 1].T)\n",
    "        dbz += dz\n",
    "\n",
    "        dWr += np.dot(dr, xs[t].T)\n",
    "        dUr += np.dot(dr, hs[t - 1].T)\n",
    "        dbr += dr\n",
    "\n",
    "        dWh += np.dot(dh_tilde_raw, xs[t].T)\n",
    "        dUh += np.dot(dh_tilde_raw, (rs[t] * hs[t - 1]).T)\n",
    "        dbh += dh_tilde_raw\n",
    "\n",
    "        dhprev = (1 - zs[t]) * dh + np.dot(Uz.T, dz)\n",
    "        dhprev += np.dot(Ur.T, dr)\n",
    "        dhprev += rs[t] * np.dot(Uh.T, dh_tilde_raw)\n",
    "        dhnext = dhprev\n",
    "\n",
    "    for dparam in [dWz, dUz, dWr, dUr, dWh, dUh, dWhy, dbz, dbr, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    return loss, dWz, dUz, dWr, dUr, dWh, dUh, dWhy, dbz, dbr, dbh, dby, hs[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"Genera texto a partir del modelo GRU entrenado.\"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        h_tilde = np.tanh(np.dot(Wh, x) + np.dot(Uh, r * h) + bh)\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# ---------- ENTRENAMIENTO ----------\n",
    "n, p = 0, 0\n",
    "# Adagrad memories\n",
    "mems = [np.zeros_like(param) for param in [Wz, Uz, Wr, Ur, Wh, Uh, Why, bz, br, bh, by]]\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "\n",
    "while True:\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        p = 0\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    loss, *grads, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0:\n",
    "        print(f'iter {n}, loss: {smooth_loss:.4f}')\n",
    "\n",
    "    # Adagrad update\n",
    "    params = [Wz, Uz, Wr, Ur, Wh, Uh, Why, bz, br, bh, by]\n",
    "    for i, (param, dparam) in enumerate(zip(params, grads)):\n",
    "        mems[i] += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mems[i] + 1e-8)\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
